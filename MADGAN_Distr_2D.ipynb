{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MADGAN_keras_Distr_2D_GPUoptimized.ipynb","provenance":[{"file_id":"1K7vx6pQ4bvQaHQpQIIb1aasPUzGok_ky","timestamp":1632418151034},{"file_id":"1hY2dSGFfGb9UEEUKU8sQYjeXb__kUFUD","timestamp":1629483313868}],"collapsed_sections":["HzKxAhSMBfBo","lLAblToISVhT","jcBSwiMhBY6s","fBkIiU-2mw9H","lEM6PrlMBZk6","NqQ0WdZcJo1s","u_mXp7LPHltf","NQ960Kc_JLxW","hm2I8u4cK29o","4FDNnzt1qxmJ"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HzKxAhSMBfBo"},"source":["# Hyper-parameters"]},{"cell_type":"code","metadata":{"id":"KG1kJCaTyILq","executionInfo":{"status":"ok","timestamp":1632476590353,"user_tz":-210,"elapsed":625,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}}},"source":["dir_name = \"Model1_2D\" #location to save the model in Google Drive"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPYGZFI8g-0C","executionInfo":{"status":"ok","timestamp":1632476590354,"user_tz":-210,"elapsed":5,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}}},"source":["n_Gen = 6 #number of generators\n","h_Dim = 128 #dimention of hidden layers\n","latent_dim =  64 # dimention of input noise\n","size_dataset =  200000 #size of dataset\n","batch_size = 128 #number of batches\n","\n","steps_per_epoch = (size_dataset//batch_size)//n_Gen"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lLAblToISVhT"},"source":["# Adding Libraries"]},{"cell_type":"code","metadata":{"id":"kUzwSXEogl5B","executionInfo":{"status":"ok","timestamp":1632476591210,"user_tz":-210,"elapsed":861,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}}},"source":["import matplotlib.pyplot as plt\n","from sklearn import mixture\n","import math\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import ReLU\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import Concatenate\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras import Model\n","from google.colab import output\n","\n","import os"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hf4eBNVlUImO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632476596216,"user_tz":-210,"elapsed":5008,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}},"outputId":"9afb639f-2dfe-42d3-a5a6-554ff002dc09"},"source":["# for saving GIF\n","!pip install pygifsicle\n","!sudo apt-get install gifsicle\n","import imageio\n","import glob\n","from pygifsicle import optimize\n","output.clear()\n","print(\"Import Done\")"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Import Done\n"]}]},{"cell_type":"markdown","metadata":{"id":"jcBSwiMhBY6s"},"source":["# To see if we have a GPU"]},{"cell_type":"code","metadata":{"id":"xCanhRwP_i9r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632476596217,"user_tz":-210,"elapsed":18,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}},"outputId":"3d287821-e15b-4560-983c-f1c73a967ad6"},"source":["print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","\n","if tf.test.gpu_device_name() == '/device:GPU:0':\n","  print(\"Using the GPU\")\n","else:\n","  print(\"Using the CPU\")"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Num GPUs Available:  0\n","Using the CPU\n"]}]},{"cell_type":"markdown","metadata":{"id":"5E683Gw4SPui"},"source":["# Mount Google drive to save model and data"]},{"cell_type":"code","metadata":{"id":"zzRNpAtj5Owg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632476596218,"user_tz":-210,"elapsed":16,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}},"outputId":"76d2ad44-d6d9-4623-d711-4e1a9ecbdd80"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","if os.path.exists(f'/content/drive/MyDrive/{dir_name}') == False:\n","    os.mkdir(f'/content/drive/MyDrive/{dir_name}')\n","    os.mkdir(f'/content/drive/MyDrive/{dir_name}/Charts')"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"fBkIiU-2mw9H"},"source":["# 2D Gaussian Dataset"]},{"cell_type":"code","metadata":{"id":"lIgGubyxzdUB","executionInfo":{"status":"ok","timestamp":1632476596218,"user_tz":-210,"elapsed":13,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}}},"source":["def dataset2d_func(size_dataset = 200000, num_mixtures = 6, radius = 5, std = 0.1, random_state = None):\n","    thetas = tf.linspace(0., 2 * math.pi, num_mixtures + 1)[:num_mixtures]\n","    centers = tf.stack([radius * tf.cos(thetas), radius * tf.sin(thetas)], axis = -1)\n","\n","    gmm = mixture.GaussianMixture(n_components = num_mixtures, covariance_type = 'diag')\n","    gmm.means_ = centers\n","    gmm.covariances_ = tf.constant([[std, std]] * num_mixtures)**2\n","    gmm.weights_ = tf.constant([1/num_mixtures] * num_mixtures)\n","    X = gmm.sample(size_dataset)\n","    return X[0]"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gq-5JHkdCWw0"},"source":["# Some Functions"]},{"cell_type":"markdown","metadata":{"id":"lEM6PrlMBZk6"},"source":["##### A function that produces uniform random noise in the range of [-1,1] of size [n_gen, batch_size, latent_dim] as the generators' input"]},{"cell_type":"code","metadata":{"id":"eHjMJzyPWHFg","executionInfo":{"status":"ok","timestamp":1632476596219,"user_tz":-210,"elapsed":14,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}}},"source":["from tensorflow_probability import distributions as tfd\n","\n","# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, batch_size, n_Gen):\n","    # Multivariate normal diagonal distribution\n","    mvn = tfd.MultivariateNormalDiag(\n","        loc=[0]*latent_dim,\n","        scale_diag=[1.0]*latent_dim)\n","    \n","    noise = []\n","    for i in range(n_Gen):\n","        # Some samples from MVN\n","        x_input = mvn.sample(batch_size)\n","        noise.append(x_input)\n","    return noise"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4oJJxQwlKH-D"},"source":["##### A callback which runs at end of each epoch to save and plot the results"]},{"cell_type":"code","metadata":{"id":"GKU1N6KsH9X8","executionInfo":{"status":"ok","timestamp":1632476596219,"user_tz":-210,"elapsed":13,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}}},"source":["class GANMonitor1(tf.keras.callbacks.Callback):\n","    def __init__(self, dataset, plot_freq = 1, num_samples = 200000, latent_dim = 64, n_Gen = 6, dir_name = 'Model'):\n","        self.dataset = dataset\n","        self.num_samples = num_samples\n","        self.latent_dim = latent_dim\n","        self.n_Gen = n_Gen\n","        self.dir_name = dir_name\n","        self.plot_freq = plot_freq\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if (epoch + 1) % self.plot_freq == 0:\n","            bin = 500\n","            random_latent_vectors = generate_latent_points(self.latent_dim, self.num_samples, self.n_Gen)\n","\n","            generated_samples = []\n","            for g in range(self.n_Gen):\n","                generated_samples.append(self.model.generators[g](random_latent_vectors[g]))\n","\n","            combined_generated_samples = tf.concat([generated_samples[g] for g in range(self.n_Gen)], axis=0)\n","\n","            _, ax = plt.subplots(nrows = 2, ncols = 4, figsize = ([12.8, 7.2]), sharey = True)\n","\n","            ax[0, 0].hist2d(self.dataset[:,0],\n","                            self.dataset[:,1], \n","                            bins=(bin, bin),\n","                            cmap=plt.cm.YlOrBr,\n","                            range = [[-150, 150], [-150, 150]],\n","                            density = True)\n","            ax[0, 0].set_aspect('equal')\n","            ax[0, 0].set_title('Dataset')\n","\n","            ax[0, 1].hist2d(generated_samples[0].numpy()[:,0],\n","                            generated_samples[0].numpy()[:,1],\n","                            bins=(bin, bin),\n","                            cmap=plt.cm.YlOrBr,\n","                            range = [[-150, 150], [-150, 150]],\n","                            density = True)\n","            ax[0, 1].set_aspect('equal')\n","            ax[0, 1].set_title('Generator 1')\n","\n","            ax[0, 2].hist2d(generated_samples[1].numpy()[:,0],\n","                            generated_samples[1].numpy()[:,1], \n","                            bins=(bin, bin),\n","                            cmap=plt.cm.YlOrBr,\n","                            range = [[-150, 150], [-150, 150]], \n","                            density = True)\n","            ax[0, 2].set_aspect('equal')\n","            ax[0, 2].set_title('Generator 2')\n","\n","            ax[0, 3].hist2d(generated_samples[2].numpy()[:,0], \n","                            generated_samples[2].numpy()[:,1], \n","                            bins=(bin, bin), \n","                            cmap=plt.cm.YlOrBr, \n","                            range = [[-150, 150], [-150, 150]],\n","                            density = True)\n","            ax[0, 3].set_aspect('equal')\n","            ax[0, 3].set_title('Generator 3')\n","\n","            ax[1, 1].hist2d(generated_samples[3].numpy()[:,0],\n","                            generated_samples[3].numpy()[:,1],\n","                            bins=(bin, bin),\n","                            cmap=plt.cm.YlOrBr, \n","                            range = [[-150, 150], [-150, 150]],\n","                            density = True)\n","            ax[1, 1].set_aspect('equal')\n","            ax[1, 1].set_title('Generator 4')\n","\n","            ax[1, 2].hist2d(generated_samples[4].numpy()[:,0],\n","                            generated_samples[4].numpy()[:,1],\n","                            bins=(bin, bin), \n","                            cmap=plt.cm.YlOrBr,\n","                            range = [[-150, 150], [-150, 150]],\n","                            density = True)\n","            ax[1, 2].set_aspect('equal')\n","            ax[1, 2].set_title('Generator 5')\n","\n","            ax[1, 3].hist2d(generated_samples[5].numpy()[:,0],\n","                            generated_samples[5].numpy()[:,1],\n","                            bins=(bin, bin), \n","                            cmap=plt.cm.YlOrBr,\n","                            range = [[-150, 150], [-150, 150]],\n","                            density = True)\n","            ax[1, 3].set_aspect('equal')\n","            ax[1, 3].set_title('Generator 6')\n","\n","            ax[1, 0].hist2d(combined_generated_samples.numpy()[:,0],\n","                            combined_generated_samples.numpy()[:,1], \n","                            bins=(bin, bin), \n","                            cmap=plt.cm.YlOrBr, \n","                            range = [[-150, 150], [-150, 150]],\n","                            density = True)\n","            ax[1, 0].set_aspect('equal')\n","            ax[1, 0].set_title('All Generators')\n","\n","            plt.subplots_adjust(hspace = 0.05, wspace = 0.1)\n","            plt.savefig(f'/content/drive/MyDrive/{self.dir_name}/Charts/chart_{(epoch + 1):04}.png', dpi=200, format=\"png\")\n","            \n","            # To show the plots in colab comment line below and uncomment the next line\n","            # plt.close()\n","            plt.show()"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NqQ0WdZcJo1s"},"source":["##### Loss function for the generators based on the MAD_GAN paper"]},{"cell_type":"code","metadata":{"id":"h9j05EQ-wcXc","executionInfo":{"status":"ok","timestamp":1632476596219,"user_tz":-210,"elapsed":11,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}}},"source":["def Generators_loss_function(y_true, y_pred):\n","    logarithm = -tf.math.log(y_pred[:,-1] + 1e-15)\n","    return tf.reduce_mean(logarithm, axis=-1)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_mXp7LPHltf"},"source":["# Defining Discriminator Model"]},{"cell_type":"code","metadata":{"id":"t01kZc3TsTMl","executionInfo":{"status":"ok","timestamp":1632476596220,"user_tz":-210,"elapsed":11,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}}},"source":["# define the standalone discriminator model\n","def define_discriminator(n_Gen, h_Dim):\n","    inp = Input(shape = (2,))\n","    x = Dense(h_Dim, input_shape=(2,), activation = 'relu')(inp)\n","    out = Dense(n_Gen+1, activation=\"softmax\")(x)\n","    model = Model(inp, out, name=\"discriminator\")\n","    return model"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NQ960Kc_JLxW"},"source":["# Defining Generators Model"]},{"cell_type":"code","metadata":{"id":"gZgacXjViZGb","executionInfo":{"status":"ok","timestamp":1632476596220,"user_tz":-210,"elapsed":11,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}}},"source":["def define_generators(n_Gen, latent_dim, h_Dim):\n","    mid_layer1 = Dense(h_Dim, name = \"hidden_1\", activation = 'relu', input_shape=(latent_dim,))\n","\n","    models = []\n","    for g in range(n_Gen):\n","        input = Input(shape=(latent_dim,), dtype = tf.float32, name=f\"input_{g}\")\n","        x = mid_layer1(input)\n","        x = BatchNormalization()(x)\n","\n","        x = Dense(h_Dim, activation = 'relu')(x)\n","        x = BatchNormalization()(x)\n","\n","        x = Dense(2, dtype = tf.float32, name=f\"generator_output_{g}\")(x)\n","\n","        models.append(Model(input, x, name = f\"generator{g}\"))\n","    return models"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hm2I8u4cK29o"},"source":["# Defining MADGAN Class for training via keras"]},{"cell_type":"code","metadata":{"id":"LyvfweJ4uoz4","executionInfo":{"status":"ok","timestamp":1632476596220,"user_tz":-210,"elapsed":10,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}}},"source":["class MADGAN(tf.keras.Model):\n","    def __init__(self, discriminator, generators, latent_dim, n_Gen):\n","        super(MADGAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generators = generators\n","        self.latent_dim = latent_dim\n","        self.n_Gen = n_Gen\n","\n","    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n","        super(MADGAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.d_loss_fn = d_loss_fn\n","        self.g_loss_fn = g_loss_fn\n","\n","    def train_step(self, data):\n","        X = data\n","        # Get the batch size\n","        batch_size = tf.shape(X)[0]\n","        # Sample random points in the latent space\n","        random_latent_vectors = generate_latent_points(self.latent_dim, batch_size//self.n_Gen, self.n_Gen)\n","        # Decode them to fake generator output\n","        x_generator = []\n","        for g in range(self.n_Gen):\n","            x_generator.append(self.generators[g](random_latent_vectors[g]))\n","\n","        # Combine them with real samples\n","        combined_samples = tf.concat([x_generator[g] for g in range(self.n_Gen)] + \n","                                     [X], \n","                                     axis=0\n","                                     )\n","\n","        # Assemble labels discriminating real from fake samples\n","        labels = tf.concat([tf.one_hot(g * tf.ones(batch_size//self.n_Gen, dtype=tf.int32), self.n_Gen + 1) for g in range(self.n_Gen)] + \n","                    [tf.one_hot(self.n_Gen * tf.ones(batch_size, dtype=tf.int32), self.n_Gen + 1)], \n","                    axis=0\n","                    )\n","\n","        # Add random noise to the labels - important trick!\n","        labels += 0.05 * tf.random.uniform(shape = tf.shape(labels), minval = -1, maxval = 1)\n","\n","        #######################\n","        # Train Discriminator #\n","        #######################\n","        \n","        # make weights in the discriminator trainable\n","        with tf.GradientTape() as tape:\n","            # Discriminator forward pass\n","            predictions = self.discriminator(combined_samples)\n","\n","            # Compute the loss value\n","            # (the loss function is configured in `compile()`)\n","            d_loss = self.d_loss_fn(labels, predictions)\n","\n","        # Compute gradients\n","        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n","\n","        # Update weights\n","        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n","\n","        #######################\n","        #   Train Generator   #\n","        #######################\n","\n","        # Assemble labels that say \"all real samples\"\n","        misleading_labels =  tf.one_hot(self.n_Gen * tf.ones(batch_size//self.n_Gen, dtype=tf.int32), self.n_Gen + 1)\n","\n","        # (note that we should *not* update the weights of the discriminator)!\n","        g_loss_list = []\n","        for g in range(self.n_Gen):\n","            with tf.GradientTape() as tape:\n","                # Generator[g] and discriminator forward pass\n","                predictions = self.discriminator(self.generators[g](random_latent_vectors[g]))\n","\n","                # Compute the loss value\n","                # (the loss function is configured in `compile()`)\n","                g_loss = self.g_loss_fn(misleading_labels, predictions)\n","\n","            # Compute gradients\n","            grads = tape.gradient(g_loss, self.generators[g].trainable_weights)\n","            # Update weights\n","            self.g_optimizer[g].apply_gradients(zip(grads, self.generators[g].trainable_weights))\n","            g_loss_list.append(g_loss)\n","\n","        mydict = {f\"g_loss{g}\": g_loss_list[g] for g in range(self.n_Gen)}\n","        mydict.update({\"d_loss\": d_loss})\n","        return mydict"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-tmPpH6dLBAV"},"source":["# Creating Model and training it"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"id":"SunFHoSGCCNg","executionInfo":{"status":"error","timestamp":1632476603359,"user_tz":-210,"elapsed":7149,"user":{"displayName":"Alireza Daeijavad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17395663091961923881"}},"outputId":"4209e99a-b9a8-48e9-8dbb-8c303816a8a7"},"source":["# Loading data\n","data = dataset2d_func(size_dataset, num_mixtures = 6, radius = 100, std = 10)\n","# Changing numpy dataset to tf.DATASET type and Shuffling dataset for training\n","dataset = tf.data.Dataset.from_tensor_slices(data) \n","dataset = dataset.repeat().shuffle(10 * size_dataset, reshuffle_each_iteration=True).batch(n_Gen * batch_size, drop_remainder=True)\n","\n","# Creating Discriminator and Generator\n","discriminator = define_discriminator(n_Gen, h_Dim)\n","generators = define_generators(n_Gen, latent_dim, h_Dim)\n","\n","# creating MADGAN\n","madgan = MADGAN(discriminator = discriminator, generators = generators, \n","                latent_dim = latent_dim, n_Gen = n_Gen)\n","\n","madgan.compile(\n","    d_optimizer = Adam(learning_rate=2e-4, beta_1=0.5),\n","    g_optimizer = [Adam(learning_rate=2e-4, beta_1=0.5) for g in range(n_Gen)],\n","    d_loss_fn = CategoricalCrossentropy(),\n","    g_loss_fn = Generators_loss_function\n",")\n","\n","# saved model directory\n","checkpoint_filepath = f'/content/drive/MyDrive/{dir_name}/checkpoint'\n","\n","# callbacks are functions that run at end of each epoch\n","my_callbacks = [\n","    # This callback is for ploting generators' output every epoch\n","    GANMonitor1(dataset = data, plot_freq = 2, num_samples = size_dataset//n_Gen, latent_dim = latent_dim, n_Gen = n_Gen, dir_name = dir_name),\n","    # This callback is for Saving the model every 15 epochs\n","    tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_filepath, save_freq = 20, save_weights_only=True),\n","]\n","\n","# # Loading previous saved model for resume training\n","# if os.path.exists(checkpoint_filepath):\n","#     madgan.load_weights(checkpoint_filepath)\n","\n","# train the model\n","madgan.fit(dataset, epochs = 500, initial_epoch = 0, steps_per_epoch = steps_per_epoch, verbose = 1, callbacks = my_callbacks)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/500\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-7bbd21b8d5a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmadgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"4FDNnzt1qxmJ"},"source":["# Saving GIF file"]},{"cell_type":"code","metadata":{"id":"x3AdDZAsoX6S"},"source":["anim_file = 'madgan.gif'\n","\n","with imageio.get_writer(f'/content/drive/MyDrive/{dir_name}/{anim_file}', mode='I') as writer:\n","    filenames = glob.glob(f'/content/drive/MyDrive/{dir_name}/Charts/chart*.png')\n","    filenames = sorted(filenames)\n","    for filename in filenames:\n","        image = imageio.imread(filename)\n","        # image = image[::4,::4,:]\n","        writer.append_data(image)\n","        writer.append_data(image)\n","    for i in range(20):\n","        writer.append_data(image)   \n","\n","# Reduce GIF size\n","optimize(f'/content/drive/MyDrive/{dir_name}/{anim_file}')"],"execution_count":null,"outputs":[]}]}